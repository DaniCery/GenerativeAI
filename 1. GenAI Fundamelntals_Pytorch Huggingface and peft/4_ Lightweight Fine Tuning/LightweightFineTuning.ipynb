{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "* Model: \n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers --quiet\n",
    "!pip install datasets --quiet\n",
    "!pip install torch --quiet\n",
    "!pip install torch torchvision torchaudio --quiet\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e78dc5ce4e04ca9b5c71884792f348b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db482b442aa496e859a504fe9ef1e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761e2dacb82e453a840044479fb627f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7a3272a86f4579926c2258e0ee60d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20409da3d4f1420c8e21e27ebf11f9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},\n",
    "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1},\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcee0b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#common parts of both models (standard and finetuned)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m splits \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimdb\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39msplits)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#ds = {split: ds for split in splits in zip(ds, splits)}\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Convert list to dictionary\u001b[39;00m\n\u001b[1;32m      8\u001b[0m ds \u001b[38;5;241m=\u001b[39m {split: ds[i] \u001b[38;5;28;01mfor\u001b[39;00m i, split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(splits)}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "#common parts of both models (standard and finetuned)\n",
    "\n",
    "splits = ['train', 'test']\n",
    "ds = load_dataset(\"imdb\", split=splits)\n",
    "\n",
    "#ds = {split: ds for split in splits in zip(ds, splits)}\n",
    "# Convert list to dictionary\n",
    "ds = {split: ds[i] for i, split in enumerate(splits)}\n",
    "\n",
    "for split in splits:\n",
    "    ds[split] = ds[split].shuffle(seed=42).select(range(500))\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation = True, padding=True)\n",
    "\n",
    "tokenized_ds = {}\n",
    "for split in splits:\n",
    "    tokenized_ds[split] = ds[split].map(preprocess_function, batched=True)\n",
    "    \n",
    "# Compute metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd17b592",
   "metadata": {},
   "outputs": [],
   "source": [
    "### with normal pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcad80c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport torch\\nfrom torch import nn, optim\\nfrom torch.utils.data import DataLoader\\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\\nfrom datasets import load_dataset\\n\\n# Initialize model and tokenizer\\nmodel = AutoModelForSequenceClassification.from_pretrained(\\n    \"distilbert-base-uncased\",\\n    num_labels=2,  # Binary classification (positive/negative)\\n    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},\\n    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1},\\n)\\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\\n\\n# Freeze base model parameters to prevent them from being updated during training\\nfor param in model.base_model.parameters():\\n    param.requires_grad = False\\n\\n# Load and prepare dataset\\nsplits = [\\'train\\', \\'test\\']\\nds = {split: load_dataset(\"imdb\", split=split) for split in splits}\\n\\n# Thin out the dataset for faster execution (only use first 500 examples from each split)\\nfor split in splits:\\n    ds[split] = ds[split].shuffle(seed=42).select(range(500))\\n\\n# Tokenization function to apply truncation and padding to the dataset\\ndef preprocess_function(examples):\\n    return tokenizer(examples[\"text\"], truncation=True, padding=True)\\n\\n# Tokenize dataset using the preprocess function\\ntokenized_ds = {split: ds[split].map(preprocess_function, batched=True) for split in splits}\\n\\n# Convert datasets to PyTorch dataloaders using DataCollatorWithPadding for dynamic padding\\ndata_collator = DataCollatorWithPadding(tokenizer)\\n\\ntrain_loader = DataLoader(tokenized_ds[\\'train\\'], batch_size=4, collate_fn=data_collator)\\ntest_loader = DataLoader(tokenized_ds[\\'test\\'], batch_size=4, collate_fn=data_collator)\\n\\n# Define loss function and optimizer\\nloss_fn = nn.CrossEntropyLoss()  # Suitable for classification tasks\\noptimizer = optim.Adam(model.parameters(), lr=2e-3)\\n\\n# Training loop\\nepochs = 1\\nmodel.train()  # Set the model to training mode\\n\\nfor epoch in range(epochs):\\n    total_loss = 0\\n    for batch in train_loader:\\n        # Move inputs and labels to the device used by the model (GPU or CPU)\\n        inputs = {k: v.to(model.device) for k, v in batch.items() if k in tokenizer.model_input_names}\\n        labels = batch[\\'labels\\'].to(model.device)\\n        \\n        # Zero the gradients before the forward pass\\n        optimizer.zero_grad()\\n        \\n        # Perform the forward pass\\n        outputs = model(**inputs)\\n        \\n        # Compute the loss\\n        loss = loss_fn(outputs.logits, labels)\\n        \\n        # Backpropagate the gradients\\n        loss.backward()\\n        \\n        # Update the model parameters\\n        optimizer.step()\\n        \\n        total_loss += loss.item()  # Accumulate the loss for reporting\\n    \\n    print(f\\'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}\\')\\n\\n# Evaluation loop\\nmodel.eval()  # Set the model to evaluation mode\\ncorrect = 0\\ntotal = 0\\n\\nwith torch.no_grad():  # Disable gradient computation for evaluation\\n    for batch in test_loader:\\n        # Move inputs and labels to the device used by the model (GPU or CPU)\\n        inputs = {k: v.to(model.device) for k, v in batch.items() if k in tokenizer.model_input_names}\\n        labels = batch[\\'labels\\'].to(model.device)\\n        \\n        # Perform the forward pass\\n        outputs = model(**inputs)\\n        \\n        # Get the predicted class with the highest logit\\n        _, predicted = torch.max(outputs.logits, 1)\\n        \\n        total += labels.size(0)  # Accumulate the total number of examples\\n        correct += (predicted == labels).sum().item()  # Accumulate the number of correct predictions\\n\\n# Compute and print the accuracy\\naccuracy = correct / total\\nprint(f\\'Test Accuracy: {accuracy * 100:.2f}%\\')\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,  # Binary classification (positive/negative)\n",
    "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},\n",
    "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1},\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Freeze base model parameters to prevent them from being updated during training\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Load and prepare dataset\n",
    "splits = ['train', 'test']\n",
    "ds = {split: load_dataset(\"imdb\", split=split) for split in splits}\n",
    "\n",
    "# Thin out the dataset for faster execution (only use first 500 examples from each split)\n",
    "for split in splits:\n",
    "    ds[split] = ds[split].shuffle(seed=42).select(range(500))\n",
    "\n",
    "# Tokenization function to apply truncation and padding to the dataset\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "# Tokenize dataset using the preprocess function\n",
    "tokenized_ds = {split: ds[split].map(preprocess_function, batched=True) for split in splits}\n",
    "\n",
    "# Convert datasets to PyTorch dataloaders using DataCollatorWithPadding for dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "train_loader = DataLoader(tokenized_ds['train'], batch_size=4, collate_fn=data_collator)\n",
    "test_loader = DataLoader(tokenized_ds['test'], batch_size=4, collate_fn=data_collator)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()  # Suitable for classification tasks\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-3)\n",
    "\n",
    "# Training loop\n",
    "epochs = 1\n",
    "model.train()  # Set the model to training mode\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        # Move inputs and labels to the device used by the model (GPU or CPU)\n",
    "        inputs = {k: v.to(model.device) for k, v in batch.items() if k in tokenizer.model_input_names}\n",
    "        labels = batch['labels'].to(model.device)\n",
    "        \n",
    "        # Zero the gradients before the forward pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Perform the forward pass\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "        \n",
    "        # Backpropagate the gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()  # Accumulate the loss for reporting\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}')\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "    for batch in test_loader:\n",
    "        # Move inputs and labels to the device used by the model (GPU or CPU)\n",
    "        inputs = {k: v.to(model.device) for k, v in batch.items() if k in tokenizer.model_input_names}\n",
    "        labels = batch['labels'].to(model.device)\n",
    "        \n",
    "        # Perform the forward pass\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # Get the predicted class with the highest logit\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        \n",
    "        total += labels.size(0)  # Accumulate the total number of examples\n",
    "        correct += (predicted == labels).sum().item()  # Accumulate the number of correct predictions\n",
    "\n",
    "# Compute and print the accuracy\n",
    "accuracy = correct / total\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06df5a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with hugginface trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:18, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.484682</td>\n",
       "      <td>0.790000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./sentiment_analysis/checkpoint-125 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_model/tokenizer_config.json',\n",
       " './fine_tuned_model/special_tokens_map.json',\n",
       " './fine_tuned_model/vocab.txt',\n",
       " './fine_tuned_model/added_tokens.json',\n",
       " './fine_tuned_model/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    # Training arguments\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./sentiment_analysis\",\n",
    "        learning_rate=2e-3,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "trainer.evaluate()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe5b31f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.48468244075775146,\n",
       " 'eval_accuracy': 0.79,\n",
       " 'eval_runtime': 8.0441,\n",
       " 'eval_samples_per_second': 62.157,\n",
       " 'eval_steps_per_second': 15.539,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3acbb4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# i instantiate the model before installing peft because it was generating errors\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},\n",
    "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce09eba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install peft --quiet\n",
    "from peft import get_peft_model, LoraConfig, PeftModelForSequenceClassification, TaskType, AutoPeftModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87479c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's examine the model structure\n",
    "'''\n",
    "(5): TransformerBlock(\n",
    "          (attention): MultiHeadSelfAttention(\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
    "            ))\n",
    "'''\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d46906da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT model configuration\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\n",
    "        \"q_lin\", \"k_lin\", \"v_lin\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "peft_model_configured = PeftModelForSequenceClassification(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:27, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.435292</td>\n",
       "      <td>0.858000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./sentiment_analysis/checkpoint-125 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./peft_fine_tuned_model/tokenizer_config.json',\n",
       " './peft_fine_tuned_model/special_tokens_map.json',\n",
       " './peft_fine_tuned_model/vocab.txt',\n",
       " './peft_fine_tuned_model/added_tokens.json',\n",
       " './peft_fine_tuned_model/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Trainer for the PEFT model\n",
    "trainer_peft = Trainer(\n",
    "    model=peft_model_configured,\n",
    "    args=TrainingArguments(\n",
    "        output_dir='./sentiment_analysis',\n",
    "        learning_rate=2e-3,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the PEFT model\n",
    "trainer_peft.train()\n",
    "\n",
    "# Evaluate the PEFT model\n",
    "trainer_peft.evaluate()\n",
    "\n",
    "# Save the fine-tuned PEFT model weights\n",
    "peft_model_configured.save_pretrained(\"./peft_fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./peft_fine_tuned_model\") #same tokenizer as before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: This movie was great, I really enjoyed it.\n",
      "Predicted Sentiment: POSITIVE\n",
      "\n",
      "Text: The plot was confusing and the acting was terrible.\n",
      "Predicted Sentiment: NEGATIVE\n",
      "\n",
      "Text: I didn't like this film at all.\n",
      "Predicted Sentiment: NEGATIVE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define some example texts for inference\n",
    "example_texts = [\n",
    "    \"This movie was great, I really enjoyed it.\",\n",
    "    \"The plot was confusing and the acting was terrible.\",\n",
    "    \"I didn't like this film at all.\",\n",
    "]\n",
    "\n",
    "# Tokenize the examples\n",
    "tokenized_input = tokenizer(example_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Move tokenized inputs to the same device as the model\n",
    "tokenized_input = {key: value.to(device) for key, value in tokenized_input.items()}\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "peft_model_configured.to(device)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = peft_model_configured(**tokenized_input)\n",
    "\n",
    "# Get predictions\n",
    "predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Map label ids to labels\n",
    "label_map = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "predicted_labels = [label_map[pred.item()] for pred in predictions]\n",
    "\n",
    "# Print results\n",
    "for text, label in zip(example_texts, predicted_labels):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted Sentiment: {label}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c87efc",
   "metadata": {},
   "source": [
    "### Reload and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea95c700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Load the saved PEFT model using AutoPeftModelForSequenceClassification\n",
    "peft_model = AutoPeftModelForSequenceClassification.from_pretrained(\"./peft_fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a3fb30d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92558956f28a440f93033880c6ea1034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reload the ds\n",
    "splits = ['train', 'test']\n",
    "ds = load_dataset(\"imdb\", split=splits)\n",
    "\n",
    "#ds = {split: ds for split in splits in zip(ds, splits)}\n",
    "# Convert list to dictionary\n",
    "ds = {split: ds[i] for i, split in enumerate(splits)}\n",
    "\n",
    "for split in splits:\n",
    "    ds[split] = ds[split].shuffle(seed=42).select(range(500))\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation = True, padding=True)\n",
    "\n",
    "tokenized_ds = {}\n",
    "for split in splits:\n",
    "    tokenized_ds[split] = ds[split].map(preprocess_function, batched=True)\n",
    "    \n",
    "# Compute metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b7e1474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-evaluation results: {'eval_loss': 0.43529194593429565, 'eval_accuracy': 0.858, 'eval_runtime': 10.1058, 'eval_samples_per_second': 49.477, 'eval_steps_per_second': 12.369}\n"
     ]
    }
   ],
   "source": [
    "# Re-evaluate the PEFT model\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./sentiment_analysis\",\n",
    "        per_device_eval_batch_size=4,\n",
    "    ),\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Re-evaluate the model\n",
    "evaluation_results = trainer.evaluate()\n",
    "print(\"Re-evaluation results:\", evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6fa324",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
