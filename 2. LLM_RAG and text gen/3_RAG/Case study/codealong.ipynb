{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce95c57",
   "metadata": {},
   "source": [
    "# Codealong Notebook\n",
    "\n",
    "Use this notebook as your \"scratch pad\" as you go through the course contents. Feel free to copy any example code and tweak it to get a better understanding of how it works!\n",
    "\n",
    "Use the **+** button or `Insert` menu to add additional code cells as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "422c2675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_base = \"https://openai.vocareum.com/v1\"\n",
    "openai.api_key = \"voc-32034342212667733781206696e1b8b3b604.60383220\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b8bc9de",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "Insufficient budget available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m ukraine_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mQuestion: When did Russia invade Ukraine?\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mAnswer: \u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m ukraine_answer \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo-instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mukraine_prompt\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/venv/lib/python3.9/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m/opt/venv/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m/opt/venv/lib/python3.9/site-packages/openai/api_requestor.py:227\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    208\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    216\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    217\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    218\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    219\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    226\u001b[0m     )\n\u001b[0;32m--> 227\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m/opt/venv/lib/python3.9/site-packages/openai/api_requestor.py:620\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    613\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    614\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    615\u001b[0m         )\n\u001b[1;32m    616\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[1;32m    617\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 620\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    626\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    627\u001b[0m     )\n",
      "File \u001b[0;32m/opt/venv/lib/python3.9/site-packages/openai/api_requestor.py:680\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    678\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[1;32m    681\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[1;32m    682\u001b[0m     )\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: Insufficient budget available"
     ]
    }
   ],
   "source": [
    "ukraine_prompt = \"\"\"\n",
    "Question: When did Russia invade Ukraine?\n",
    "Answer: \n",
    "\"\"\"\n",
    "\n",
    "ukraine_answer = openai.Completion.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=ukraine_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fd35ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#response type\n",
    "'''<OpenAIObject text_completion id=cmpl-6hgvbhgyasdfkj at 0x7f765af9d8b0> JSON: {\n",
    "  \"choices\": [\n",
    "    {\n",
    "      \"finish_reason\": \"stop\",\n",
    "      \"index\": 0,\n",
    "      \"logprobs\": null,\n",
    "      \"text\": \"Russia invaded Ukraine in 2014.\"\n",
    "    }\n",
    "  ],\n",
    "  \"created\": 1679090273,\n",
    "  \"id\": \"cmpl-6hgvbhgyasdfkj\",\n",
    "  \"model\": \"gpt-3.5-turbo-instruct\",\n",
    "  \"object\": \"text_completion\",\n",
    "  \"usage\": {\n",
    "    \"completion_tokens\": 6,\n",
    "    \"prompt_tokens\": 14,\n",
    "    \"total_tokens\": 20\n",
    "  }\n",
    "}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82353e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "ukraine_answer[\"choices\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86da1ad2",
   "metadata": {},
   "source": [
    "## Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72084745",
   "metadata": {},
   "source": [
    "### Loading the Data with `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7906568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Base endpoint URL for the Wikipedia API\n",
    "endpoint = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "# Set the parameters for the API request\n",
    "params = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"prop\": \"extracts\",\n",
    "    \"titles\": \"2022\",\n",
    "    \"explaintext\": True,\n",
    "    \"exsectionformat\": \"plain\",\n",
    "    \"exlimit\": \"max\"\n",
    "}\n",
    "\n",
    "# Make the request to the Wikipedia API\n",
    "response = requests.get(endpoint, params=params)\n",
    "data = response.json()\n",
    "\n",
    "# Extract and print the response data\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4011316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract the page content\n",
    "pages = data['query']['pages']\n",
    "content = \"\"\n",
    "for page_id, page_info in pages.items():\n",
    "    content = page_info['extract']\n",
    "\n",
    "# Process the content to extract events\n",
    "events = []\n",
    "for line in content.split('\\n'):\n",
    "    if line.startswith('* '):  # Each event might be listed with a bullet point\n",
    "        events.append(line[2:])\n",
    "\n",
    "# Convert to a pandas DataFrame for better readability\n",
    "events_df = pd.DataFrame(events, columns=[\"Event\"])\n",
    "print(events_df.head(10))  # Display the first 10 events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37f65ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7eff2773",
   "metadata": {},
   "source": [
    "### Creating an Embeddings Index with `openai.Embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9226bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_NAME = \"text-embedding-ada-002\"\n",
    "# Generate the embedding response\n",
    "sentences =[]\n",
    "for sentence in df.text.values(:)\n",
    "    response = openai.Embedding.create(\n",
    "    engine = EMBEDDING_MODEL_NAME,\n",
    "    input = USER_QUESTION\n",
    "    )\n",
    "    sentences.append(response)\n",
    "print(response.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fccab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings for dataframe\n",
    "df[\"embeddings\"] = sentences\n",
    "df[\"embeddings\"] = df[\"embeddings\"].apply(eval).apply(np.array)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c6f74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_QUESTION = ukraine_answer\n",
    "\n",
    "# Extract the embeddings from the response\n",
    "question_embeddings = response[\"data\"][0][\"embedding\"]\n",
    "\n",
    "#print(question_embeddings[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7b7331",
   "metadata": {},
   "source": [
    "## Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2152dad6",
   "metadata": {},
   "source": [
    "### Finding Relevant Data with Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbd7bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.embeddings_utils import distances_from_embeddings\n",
    "\n",
    "# Create a list containing the distances from question_embeddings\n",
    "distances = distances_from_embeddings(question_embeddings, df[\"embeddings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581fc320",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"distances\"] = distances\n",
    "df.sort_values(by=\"distances\", ascending=True, inplace=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec62d71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "699486f6",
   "metadata": {},
   "source": [
    "## Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3716d6",
   "metadata": {},
   "source": [
    "### Tokenizing with `tiktoken`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e969fa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab51ffec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0888179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a65d6d19",
   "metadata": {},
   "source": [
    "### Composing a Custom Text Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c35486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(question, df, max_token_count):\n",
    "    \"\"\"\n",
    "    Given a question and a dataframe containing rows of text and their\n",
    "    embeddings, return a text prompt to send to a Completion model\n",
    "    \"\"\"\n",
    "    # Create a tokenizer that is designed to align with our embeddings\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    # Count the number of tokens in the prompt template and question\n",
    "    prompt_template = \"\"\"\n",
    "                Answer the question based on the context below, and if the question\n",
    "                can't be answered based on the context, say \"I don't know\"\n",
    "\n",
    "                Context: \n",
    "\n",
    "                {}\n",
    "\n",
    "                ---\n",
    "\n",
    "                Question: {}\n",
    "                Answer:\"\"\"\n",
    "\n",
    "    current_token_count = len(tokenizer.encode(prompt_template)) + \\\n",
    "                            len(tokenizer.encode(question))\n",
    "\n",
    "    context = []\n",
    "    for text in get_rows_sorted_by_relevance(question, df)[\"text\"].values:\n",
    "\n",
    "        # Increase the counter based on the number of tokens in this row\n",
    "        text_token_count = len(tokenizer.encode(text))\n",
    "        current_token_count += text_token_count\n",
    "\n",
    "        # Add the row of text to the list if we haven't exceeded the max\n",
    "        if current_token_count <= max_token_count:\n",
    "            context.append(text)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return prompt_template.format(\"\\n\\n###\\n\\n\".join(context), question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86f7502",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt = create_prompt(USER_QUESTION, df, 4097)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c482c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8b8f5dd",
   "metadata": {},
   "source": [
    "## Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495a549a",
   "metadata": {},
   "source": [
    "### Getting a Custom Q&A Response with `openai.Completion`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3913334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic example code (not needed for this tutorial)\n",
    "COMPLETION_MODEL_NAME=\"gpt-3.5-turbo-instruct\",\n",
    "max_answer_tokens=150\n",
    "\n",
    "try:\n",
    "    response = openai.Completion.create(\n",
    "        model=COMPLETION_MODEL_NAME,\n",
    "        prompt=custom_prompt,\n",
    "        max_tokens=max_answer_tokens\n",
    "    )\n",
    "    return response[\"choices\"][0][\"text\"].strip()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626eb389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e3d0e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
